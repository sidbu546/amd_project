#!/bin/bash

# Setup script for running Llama 2 7B with vLLM on AMD Mi300 GPU

echo "======================================"
echo "vLLM Llama 2 7B Setup Script"
echo "======================================"

# Step 1: Set Hugging Face token (replace with your token)
echo "Setting up Hugging Face authentication..."
export HF_TOKEN="<>"

# Step 2: Set environment variables for AMD GPU
export VLLM_USE_TRITON_FLASH_ATTN=0  # Use CK Flash Attention for AMD
export HSA_OVERRIDE_GFX_VERSION=9.4.2  # For Mi300 series

# Step 3: Start vLLM server
echo "Starting vLLM server with Qwen/Qwen2-1.5B-Instruct.."
vllm serve Qwen/Qwen2-1.5B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --served-model-name llama-2-7b \
  --dtype auto \
  --max-model-len 4096 &

# Wait for server to start
echo "Waiting for server to start (30 seconds)..."
sleep 110 # may need to increase depending on models weight loading 

# Step 4: Test the server
echo "======================================"
echo "Testing vLLM server..."
echo "======================================"

curl -X POST http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-2-7b",
    "prompt": "What is the capital of France?",
    "max_tokens": 50,
    "temperature": 0.7
  }'

echo ""
echo "======================================"
echo "Server is running on http://localhost:8000"
echo "To stop the server, run: pkill -f vllm"
echo "======================================"
